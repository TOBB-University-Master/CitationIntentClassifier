import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split, Dataset
from transformers import get_scheduler, AutoTokenizer
from torch.optim import AdamW
from tqdm import tqdm
import os
import logging
import json
import argparse  # Komut satÄ±rÄ±ndan argÃ¼man almak iÃ§in

# Orijinal betiÄŸinizdeki yardÄ±mcÄ± sÄ±nÄ±flarÄ± ve fonksiyonlarÄ± import edin
from train_v1_1 import setup_logging, evaluate
from dataset import CitationDataset
from generic_model import TransformerClassifier


def continue_training(trial_dir, total_epochs):
    """
    Belirtilen bir deneme klasÃ¶rÃ¼ndeki checkpoint'ten eÄŸitimi devam ettirir.
    """
    # 1. AdÄ±m: Orijinal yapÄ±landÄ±rmayÄ± yÃ¼kle
    config_path = os.path.join(trial_dir, "training_config.json")
    if not os.path.exists(config_path):
        print(f"HATA: {config_path} bulunamadÄ±. LÃ¼tfen geÃ§erli bir deneme klasÃ¶rÃ¼ belirtin.")
        return

    with open(config_path, 'r') as f:
        config = json.load(f)

    # Yeni toplam epoch sayÄ±sÄ±nÄ± config'e ekle
    config['total_epochs'] = total_epochs

    # LoglamayÄ± aynÄ± klasÃ¶rdeki log dosyasÄ±na ekleme yaparak devam ettir
    setup_logging(log_file=os.path.join(trial_dir, "training.log"))

    logging.info("==========================================================")
    logging.info(f"EÄÄ°TÄ°ME DEVAM EDÄ°LÄ°YOR: {trial_dir}")
    logging.info(f"Yeni hedef epoch sayÄ±sÄ±: {total_epochs}")
    logging.info("==========================================================")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.manual_seed(config["seed"])

    # 2. AdÄ±m: Tokenizer, Veri Seti ve DataLoader'larÄ± yeniden oluÅŸtur
    # (Orijinal veri bÃ¶lÃ¼nmesini saÄŸlamak iÃ§in aynÄ± seed kullanÄ±lÄ±r)
    tokenizer = AutoTokenizer.from_pretrained(trial_dir)  # KaydedilmiÅŸ tokenizer'Ä± yÃ¼kle
    full_dataset = CitationDataset(tokenizer=tokenizer, mode="labeled", csv_path=config['data_path'])

    num_labels = len(full_dataset.get_label_names())
    label_names_list = full_dataset.get_label_names()

    generator = torch.Generator().manual_seed(config["seed"])
    train_val_size = int(0.8 * len(full_dataset))
    test_size = len(full_dataset) - train_val_size
    train_val_dataset, _ = random_split(full_dataset, [train_val_size, test_size], generator=generator)

    train_size = int(0.85 * len(train_val_dataset))
    val_size = len(train_val_dataset) - train_size
    train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size], generator=generator)

    train_loader = DataLoader(train_dataset, batch_size=config["batch_size"], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config["batch_size"])

    # 3. AdÄ±m: Modeli, Optimizer'Ä± ve Scheduler'Ä± yeniden oluÅŸtur
    model = TransformerClassifier(model_name=config["model_name"], num_labels=num_labels)
    model.transformer.resize_token_embeddings(len(tokenizer))
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"])
    num_training_steps = len(train_loader) * total_epochs  # Yeni epoch sayÄ±sÄ±na gÃ¶re ayarla
    lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0,
                                 num_training_steps=num_training_steps)
    criterion = nn.CrossEntropyLoss()

    # 4. AdÄ±m: Checkpoint'i yÃ¼kle
    checkpoint_path = os.path.join(trial_dir, "checkpoint.pt")
    if not os.path.exists(checkpoint_path):
        logging.error(f"HATA: {checkpoint_path} bulunamadÄ±. Bu deneme hiÃ§ eÄŸitilmemiÅŸ olabilir.")
        return

    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    lr_scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
    best_val_acc = checkpoint.get("best_val_acc", 0.0)
    logging.info(f"Checkpoint yÃ¼klendi. {start_epoch}. epoch'tan devam ediliyor.")

    if start_epoch >= total_epochs:
        logging.warning(
            f"Model zaten {start_epoch} epoch eÄŸitilmiÅŸ. Yeni hedef epoch sayÄ±sÄ± ({total_epochs}) daha dÃ¼ÅŸÃ¼k veya eÅŸit. EÄŸitim yapÄ±lmayacak.")
        return

    # 5. AdÄ±m: EÄŸitim dÃ¶ngÃ¼sÃ¼ne devam et
    for epoch in range(start_epoch, total_epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{total_epochs}", leave=True)

        for batch in progress_bar:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            intent_labels = batch["label"].to(device)
            intent_logits = model(input_ids, attention_mask)
            loss = criterion(intent_logits, intent_labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            total_loss += loss.item()
            progress_bar.set_postfix(loss=f"{total_loss / (progress_bar.n + 1):.4f}")

        # ... (DeÄŸerlendirme ve kaydetme mantÄ±ÄŸÄ± train_v1.py ile aynÄ±)
        avg_train_loss = total_loss / len(train_loader)
        intent_val_acc, intent_report = evaluate(model, val_loader, device, label_names_list)
        logging.info(
            f"Epoch {epoch + 1} TamamlandÄ±. Ortalama EÄŸitim KaybÄ±: {avg_train_loss:.4f}, DoÄŸrulama BaÅŸarÄ±mÄ±: {intent_val_acc:.4f}")

        if intent_val_acc > best_val_acc:
            best_val_acc = intent_val_acc
            logging.info(f"ğŸš€ Yeni en iyi doÄŸrulama baÅŸarÄ±mÄ±: {best_val_acc:.4f}. Model kaydediliyor...")
            torch.save(model.state_dict(), os.path.join(trial_dir, "best_model.pt"))

        torch.save({
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": lr_scheduler.state_dict(),
            "best_val_acc": best_val_acc
        }, checkpoint_path)

    logging.info("EÄŸitime devam etme iÅŸlemi tamamlandÄ±.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Bir Optuna denemesinin eÄŸitimine devam et.")
    parser.add_argument("--trial_dir", type=str, required=True,
                        help="Devam edilecek denemenin klasÃ¶r yolu (Ã¶rn: checkpoints_v1/bert-base-turkish-cased/trial_12/)")
    parser.add_argument("--total_epochs", type=int, required=True, help="Toplamda kaÃ§ epoch'a kadar eÄŸitileceÄŸi.")

    args = parser.parse_args()

    continue_training(args.trial_dir, args.total_epochs)